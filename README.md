# README: Bus Ticket Marketplace Data Processing Pipeline

## Overview

This project implements a robust data pipeline to process JSON event data generated by users on a bus ticket marketplace platform. The platform tracks searches for bus tickets and stores event details, such as travel options, available seats, pricing, and routes. The objective is to transform, clean, and aggregate these events to extract insights, such as the most popular bus routes, average prices by route and service class, and the total available seats for each company and route.

The pipeline is implemented in PySpark, using Object-Oriented Programming (OOP) principles for better maintainability and scalability.

## Features

- **Reading JSON Data**: The pipeline reads JSON files containing search events, extracts and flattens nested data, and converts it into a Spark DataFrame.
- **Data Transformation**: Additional columns, such as `departure_datetime`, `arrival_datetime`, and `route`, are created to facilitate further analysis.
- **Filtering**: Events are filtered based on future departure dates and available seats.
- **Aggregation**: Aggregates are performed to calculate:
  - Average price by route and service class.
  - Total available seats by route and travel company.
  - Most popular route by company (based on the lowest sum of available seats).
- **Data Writing**: The processed data is written to Parquet files, partitioned by origin and detination state.

## Directory Structure

```bash
project/
│
├── input_data.json       # Example input JSON data
├── results/              # Output folder containing aggregated Parquet files
│   ├── general/
│   ├── avg/
│   ├── sum/
│   └── pop/
│
├── classes.py               # Main script that executes the pipeline
└── requirements.txt      # Python dependencies
```

## Installation

To run this project, ensure you have the following dependencies installed. You can install them via `pip` using the `requirements.txt` file.

### Dependencies

- Apache Spark 3.x
- Python 3.x
- PySpark

Use the following command to install dependencies:

```bash
pip install -r requirements.txt
```

## Usage

To run the pipeline, execute the `classes.py` script. The script processes the input JSON file (`input_data.json`), performs transformations and aggregations, and writes the results to the `results` folder.

### Running the Pipeline

```bash
python classes.py
```

### Parameters

- **Input Data**: The pipeline expects the JSON data to be located in the `input_data.json` file.
- **Output Data**: The processed results will be saved in Parquet format inside the `results` folder, partitioned by `originState` and `destinationState`.

### Data Flow

1. **EventProcessor**: 
   - Reads and normalizes the JSON data.
   - Adds necessary columns like `departure_datetime`, `arrival_datetime`, and `route`.
   - Filters the data based on departure date and available seats.
   
2. **Aggregator**:
   - Computes three key metrics:
     - Average price by route and service class.
     - Total available seats by route and travel company.
     - Most popular route per travel company based on the lowest sum of available seats.
   
3. **Writer**:
   - Saves the aggregated results to Parquet format, partitioned by `originState` and `destinationState`.

### Example Output

After running the pipeline, the following DataFrames are generated and stored as Parquet files:

- **General Data**: All processed events, including calculated columns.
- **Avg**: Average price by route and service class.
- **Sum**: Total available seats by route and company.
- **Pop**: Most popular routes per company (with the lowest available seats).

Example output for **Most Popular Routes**:

| route                 | travelCompanyName   |
|-----------------------|---------------------|
| BELO HORIZONTE BRASÍLIA | VIAGEM CONFORTÁVEL  |
| RIO DE JANEIRO SÃO PAULO | Rapido Vermelho    |

## Classes and Methods

### EventProcessor

The `EventProcessor` class handles the extraction and transformation of raw JSON event data.

- **`read_json(path: str) -> DataFrame`**: Reads a JSON file from the given path and flattens the nested structure.
- **`create_columns(df: DataFrame) -> DataFrame`**: Adds new columns to the DataFrame, such as `departure_datetime`, `arrival_datetime`, and `route`.
- **`filter(df: DataFrame) -> DataFrame`**: Filters rows based on future departure dates and available seats.
- **`process_events(path: str) -> DataFrame`**: Combines the `read_json`, `create_columns`, and `filter` methods to process the data and return a clean DataFrame.

### Aggregator

The `Aggregator` class performs various aggregation operations on the DataFrame.

- **`avg_price_per_route_and_class(df: DataFrame) -> DataFrame`**: Computes the average price by route and service class.
- **`sum_seat_aviable_per_route_and_company(df: DataFrame) -> DataFrame`**: Calculates the total available seats by route and company.
- **`most_popular_route_per_company(df: DataFrame) -> DataFrame`**: Identifies the most popular route for each company based on the lowest sum of available seats.
- **`aggregate_data(df: DataFrame) -> DataFrame`**: Executes all aggregation functions and returns the final DataFrames.

### Writer

The `Writer` class is responsible for writing the processed DataFrame to Parquet files.

- **`write_data(name: str, df: DataFrame) -> None`**: Saves the DataFrame in Parquet format, partitioned by `originState` and `destinationState`.

## Conclusion

This data processing pipeline allows for the efficient extraction and transformation of bus ticket search event data. The use of PySpark ensures that the pipeline can handle large datasets and scale across distributed systems. The Object-Oriented approach promotes code reusability and modularity, making the solution maintainable and easy to extend.